{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990c763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar SparkSession\n",
    "spark = SparkSession.builder.appName(\"MiApp\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo CSV\n",
    "csv_path = \"country_wise_latest.csv\"\n",
    "\n",
    "# Cargar datos desde CSV utilizando RDD\n",
    "rdd = spark.sparkContext.textFile(csv_path)\n",
    "# Convertir RDD a DataFrame\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda row: row != header)  # Filtrar la fila de encabezado\n",
    "rdd = rdd.map(lambda row: row.split(\",\"))    # Dividir por comas\n",
    "df = spark.createDataFrame(rdd, header.split(\",\"))\n",
    "\n",
    "# Especifica la ruta donde deseas guardar el archivo Parquet\n",
    "parquet_output_path = \"country_wise_latest.parquet\"\n",
    "\n",
    "# Guardar el DataFrame en formato Parquet\n",
    "df.write.parquet(parquet_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ff2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Ruta del archivo CSV\n",
    "csv_path = \"country_wise_latest.csv\"\n",
    "\n",
    "# Cargar datos desde CSV utilizando RDD\n",
    "rdd = spark.sparkContext.textFile(csv_path)\n",
    "# Convertir RDD a DataFrame\n",
    "header = rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e9a6614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.3.0-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\spark\\spark-3.3.0-bin-hadoop3\")\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae74bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"covid\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc\n",
    "# Ruta del archivo CSV\n",
    "csv_path = \"country_wise_latest.csv\"\n",
    "\n",
    "# Cargar datos desde CSV utilizando RDD\n",
    "rdd = spark.sparkContext.textFile(csv_path)\n",
    "# Convertir RDD a DataFrame\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda row: row != header)  # Filtrar la fila de encabezado\n",
    "rdd = rdd.map(lambda row: row.split(\",\"))    # Dividir por comas\n",
    "df = spark.createDataFrame(rdd, header.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27750f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo CSV descargado\n",
    "csv_file_path = \"country_wise_latest.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def read_cvs_past_top_parquet(path):\n",
    "        for filename in os.listdir(path):\n",
    "            \n",
    "            df = spark.read.csv(path+'/'+filename, header = True)\n",
    "            print(path+'/'+filename+'/')\n",
    "            f=filename.split('.')[0]\n",
    "            #print(filename.split('.')[0])\n",
    "            df.repartition(2).write.mode('overwrite').parquet('tmp/'+f+'/')\n",
    "        #df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8664ab89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-B661AT2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>covid</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=covid>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"covid\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff3085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if '.csv' in filename :\n",
    "        #print(filename)\n",
    "        # Leer el archivo CSV en un DataFrame de Spark\n",
    "        \n",
    "        df = spark.read.option(\"header\", \"true\").csv(filename)\n",
    "        # Guardar el DataFrame en formato Parquet\n",
    "        filename=filename.split('.')[0]\n",
    "        parquet_output_path = f'tmp/'+filename+'/'\n",
    "        df.repartition(2).write.mode(\"overwrite\").parquet(parquet_output_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629a233b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/Pc/Documents/covid_19/customerheaders.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.IOException: Input path does not exist: file:/C:/Users/Pc/Documents/covid_19/customerheaders.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomerheaders.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(headers)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/Pc/Documents/covid_19/customerheaders.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.IOException: Input path does not exist: file:/C:/Users/Pc/Documents/covid_19/customerheaders.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "headers = sc.textFile('country_wise_latest.txt').map(lambda line: line.split(\":\")).collect()\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "618f2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField('date', StringType(), True)\n",
      "StructField('country_region ', StringType(), True)\n",
      "StructField('confirmed', IntegerType(), True)\n",
      "StructField('recovered', IntegerType(), True)\n",
      "StructField('actived', IntegerType(), True)\n",
      "StructField('new_cases', IntegerType(), True)\n",
      "StructField('new_deaths ', IntegerType(), True)\n",
      "StructField('new_recovered', IntegerType(), True)\n",
      "StructField('who_region', StringType(), True)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType, StringType,DateType\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "def strToType(str):\n",
    "  if str == 'int':\n",
    "    return IntegerType()\n",
    "  elif str == 'double':\n",
    "    return DoubleType()\n",
    "  elif field_type == DateType():\n",
    "            parsed_tokens.append(to_date(token, 'yyyy-MM-dd'))  # Convertir la cadena a fecha \n",
    "  else:\n",
    "    return StringType()\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(t[0], strToType(t[1]), True) for t in headers]\n",
    ")\n",
    "\n",
    "for item in schema:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "398bbb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-01-22,Afghanistan,0,0,0,0,0,0,0,Eastern Mediterranean', '2020-01-22,Albania,0,0,0,0,0,0,0,Europe', '2020-01-22,Algeria,0,0,0,0,0,0,0,Africa', '2020-01-22,Andorra,0,0,0,0,0,0,0,Europe']\n"
     ]
    }
   ],
   "source": [
    "textFileRecordsRDD = sc.textFile('full_grouped.csv')\n",
    "header = textFileRecordsRDD.first()\n",
    "textFileRecordsRDD = textFileRecordsRDD.filter(lambda row: row != header)\n",
    "\n",
    "# Toma los primeros 4 registros (excluyendo la cabecera)\n",
    "resultados = textFileRecordsRDD.take(4)\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b86859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-01-22', 'Afghanistan', 0, 0, 0, 0, 0, 0, '0']\n",
      "['2020-01-22', 'Albania', 0, 0, 0, 0, 0, 0, '0']\n",
      "['2020-01-22', 'Algeria', 0, 0, 0, 0, 0, 0, '0']\n",
      "['2020-01-22', 'Andorra', 0, 0, 0, 0, 0, 0, '0']\n"
     ]
    }
   ],
   "source": [
    "def parseLine(line):\n",
    "          tokens = zip(line.split(\",\"), headers)\n",
    "          parsed_tokens = []\n",
    "          for token in tokens:\n",
    "            token_type = token[1][1]\n",
    "            print('token_type = ', token[0])\n",
    "            if token_type == 'double':\n",
    "              parsed_tokens.append(float(token[0]))\n",
    "            elif token_type == 'int':\n",
    "              parsed_tokens.append(int(token[0]))\n",
    "            else:\n",
    "              parsed_tokens.append(token[0])\n",
    "          return parsed_tokens\n",
    "\n",
    "\n",
    "records = textFileRecordsRDD.map(parseLine)\n",
    "\n",
    "for item in records.take(4):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6605941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(date='2020-01-22', country_region ='Afghanistan', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0'), Row(date='2020-01-22', country_region ='Albania', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0'), Row(date='2020-01-22', country_region ='Algeria', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0'), Row(date='2020-01-22', country_region ='Andorra', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0'), Row(date='2020-01-22', country_region ='Angola', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0'), Row(date='2020-01-22', country_region ='Antigua and Barbuda', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0'), Row(date='2020-01-22', country_region ='Argentina', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0'), Row(date='2020-01-22', country_region ='Armenia', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0'), Row(date='2020-01-22', country_region ='Australia', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0'), Row(date='2020-01-22', country_region ='Austria', confirmed=0, recovered=0, actived=0, new_cases=0, new_deaths =0, new_recovered=0, who_region='0')]\n"
     ]
    }
   ],
   "source": [
    "# df = sc.createDataFrame(records, schema)\n",
    "df = spark.createDataFrame(records, schema)\n",
    "# Muestra al menos una línea de datos del DataFrame utilizando el método 'take'\n",
    "data = df.take(10)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
